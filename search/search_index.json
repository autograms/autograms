{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AutoGRAMS","text":"<p>Welcome to the official documentation for AutoGRAMS, the Autonomous Graphical Agent Modeling Software. AutoGRAMS is a Python framework designed to transform chatbots into dynamic, stateful programs. It leverages Python\u2019s full power, allowing developers to craft intelligent conversational agents with advanced control over dialogue flow, memory, and program state. </p> <p>By representing a chatbot as a continuously running program, AutoGRAMS enables you to precisely control the prompts and memory at different points in the conversation. Decisions made during conversations directly influence the program\u2019s execution path, variables persist as the chatbot\u2019s memory, and conversations seamlessly adapt based on the program\u2019s current state.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Modern conversational AI systems often face challenges in guiding users through structured, multi-turn interactions. For instance, an educational tutor chatbot may need to follow a defined script or dynamically adjust its path based on user input. Similarly, a virtual recruiter might want to branch into different conversational routes depending on the user's qualifications. Standard AI models often lose coherence in such scenarios, struggling to maintain both the context and structure of a conversation over time.</p> <p>AutoGRAMS addresses these challenges by treating chatbots as dynamic programs. Conversations are implicitly modeled as graphs, with each node representing a specific conversational state and transitions between nodes representing the dialogue flow. This approach unlocks a range of powerful features that make AutoGRAMS uniquely suited for building complex, stateful agents:</p>"},{"location":"#key-features","title":"Key Features","text":"<ol> <li> <p>Continuously Running Programs    AutoGRAMS enables you to write chatbots as Python programs that persist their state across user interactions. When using the <code>@autograms_function</code> decorator, the chatbot resumes execution exactly where it left off after each conversational exchange with the user, allowing for highly dynamic, context-aware conversations.</p> </li> <li> <p>Save and Resume Program State    AutoGRAMS allows you to serialize the program\u2019s complete state, including the call stack, local and global variables, and control flow position. This means you can pause the program after any conversational turn, save it, and later reload it to continue from the exact same state.</p> </li> <li> <p>Dynamic Control Flow    AutoGRAMS introduces constructs like <code>GOTO</code>, <code>TRANSITION</code>, and <code>RETURNTO</code> to enable flexible navigation through your chatbot\u2019s logic. These are used to jump to special autograms nodes that you define with addresses.</p> </li> <li> <p>AI-Driven Decisions    Integrate LLM-powered decision-making directly into your chatbot\u2019s workflow. Functions like <code>yes_or_no</code> and <code>multiple_choice</code> guide the program\u2019s branching logic based on user input or internal criteria, enabling intelligent and dynamic conversation flow.</p> </li> <li> <p>Memory Management    AutoGRAMS automatically manages memory across turns, preserving variables and conversation history. The memory object tracks context, ensuring smooth continuity and coherence in multi-turn dialogues.</p> </li> <li> <p>Visualize Conversation Flow    AutoGRAMS can compile your chatbot\u2019s logic into detailed graph visualizations. These visualizations map out the program\u2019s states and decision points, helping you design, debug, and understand the flow of your conversations at a glance.</p> </li> </ol>"},{"location":"#how-autograms-works","title":"How AutoGRAMS Works","text":"<p>At its core, AutoGRAMS reimagines chatbots as programs that can be paused, serialized, and resumed. Each conversation is a journey through the program's logic, with the chatbot's state representing its memory and the current position in the program determining its next actions. Here\u2019s how it comes together:</p> <ul> <li> <p>Pause and Resume: Whenever the chatbot provides a reply, AutoGRAMS pauses the program execution. The current state is saved, and the program can be reloaded later to continue seamlessly from the same point.</p> </li> <li> <p>Dynamic Graph Representation: Conversations are represented as a graph of nodes (states) and edges (transitions). AutoGRAMS allows precise navigation of this graph using dynamic constructs like <code>GOTO</code>.</p> </li> <li> <p>LLM-Powered Decisions: By embedding decision-making functions directly into the program, AutoGRAMS ensures that the chatbot\u2019s actions align with its conversational objectives, guided by both user input and AI-driven logic.</p> </li> </ul> <p>Together, these capabilities enable a new level of flexibility and control in chatbot development, making AutoGRAMS the ideal framework for building advanced conversational agents.</p> <p>Welcome to the official documentation of the AutoGRAMS framework. AutoGRAMS, or Autonomous Graphical Agent Modeling Software, is a Python library designed to streamline the creation of intelligent, stateful chatbots. It builds on the concept of conversational agents as dynamic systems with complex state management, providing powerful tools to craft flexible, controlled dialogues. Unlike its earlier version, which introduced a custom programming language, this new iteration leverages pure Python for maximum flexibility, integration, and ease of use.</p>"},{"location":"#introduction_1","title":"Introduction","text":"<p>Modern conversational AI systems often struggle to lead multi-turn conversations along specific paths or adhere to structured scripts. For example, imagine building an educational chatbot or a virtual recruiter. You may want the agent to guide the user through a carefully planned sequence of interactions. However, standard language models tend to lose focus over long conversations or struggle to maintain a predefined conversational structure. </p> <p>AutoGRAMS solves this problem by allowing you to define these structured interactions directly in Python. It offers tools to build chatbot workflows using common programming constructs like loops, conditionals, and stateful branching. Conversations are implicitly represented as a dynamic graph, where each node corresponds to a specific point in the conversation. Here's what makes AutoGRAMS unique:</p>"},{"location":"#key-features_1","title":"Key Features","text":"<ol> <li> <p>Stateful Conversations with Serialization    AutoGRAMS allows chatbots to save and restore their entire program state between turns, enabling mid-conversation reloads. By using the <code>@autograms_function</code> decorator, functions can automatically handle program state, allowing the chatbot to seamlessly continue from the exact point it left off after each reply.</p> </li> <li> <p>Automatic Conversation History and Prompt Management    Every reply in AutoGRAMS is generated within a memory context that tracks conversation history, manages system prompts, and seamlessly integrates new user inputs. This memory is thread-specific and ensures isolation between concurrent sessions.</p> </li> <li> <p>Decision-Making Functions    AutoGRAMS offers specialized functions like <code>yes_or_no</code> and <code>multiple_choice</code>, designed to streamline branching logic. These functions allow the chatbot to make decisions based on user input or internal criteria and adjust the conversation flow accordingly.</p> </li> <li> <p>Advanced Control Flow (GOTO and RETURNTO)    For more complex chatbot behaviors, AutoGRAMS introduces control flow constructs like <code>GOTO</code> and <code>RETURNTO</code>, enabling precise navigation across conversation states. These constructs are especially useful for state-driven chatbots that require jumping between different points in a workflow.</p> </li> <li> <p>Integration with AI APIs    AutoGRAMS simplifies calling AI APIs (e.g., OpenAI) to handle tasks like text generation, classification, and embedding extraction. These functions are designed to work seamlessly with the memory and state management system.</p> </li> </ol>"},{"location":"#installation-and-requirements","title":"Installation and Requirements","text":"<p>You can install AutoGRAMS via pip, either with </p> <pre><code>pip install autograms\n</code></pre> <p>or <code>pip install .</code> from the root directory</p> <p>Requirements: - Python 3.9+ - Graphviz (optionial, for visualizing conversation graphs). On Linux:   <pre><code>sudo apt install graphviz\n</code></pre> - OpenAI API Key (optional, for openai api--can also use other models through proxy). Set it in your environment:   <pre><code>export OPENAI_API_KEY=[your_key]\n</code></pre>   Alternatively, you can store API keys in an <code>api_keys.json</code> file for flexibility.</p>"},{"location":"#simple-getting-started-example","title":"Simple \"getting started\" example","text":"<p>Here we'll walk through setting up and running a basic chatbot using Autograms. The example chatbot prompts the user about their interest in AI and dynamically adjusts the conversation based on their response. We'll also generate an interactive graph to visualize the flow of the chatbot.</p>"},{"location":"#step-1-coding-a-simple-chatbot","title":"Step 1: Coding a simple chatbot","text":"<p>Let's create a new file called <code>simple_example.py</code> to code a very simple autograms chatbot that asks the user if they would like to know more about the latest advances in AI, and tells them a bit about AutoGRAMS if they say yes. <pre><code>from autograms.nodes import reply, reply_instruction\nfrom autograms.functional import yes_or_no\nfrom autograms import autograms_function\n\n#this decorator allows the function to have special control flow behavior such as temporarily returning replies\n@autograms_function()\ndef chatbot():\n\n    #model goes first here, we have a fixed introductory message\n    reply(\"Would you like me to tell you more about the latest advances in AI?\", ADDRESS=\"ask_question\")\n    #program will continue from this point after first user reply \n\n    #The agent decides whether it thinks the user wants to talk about AI based on their response  \n    user_wants_ai = yes_or_no(\"does the user want to talk about AI?\")\n\n    if user_wants_ai:\n        #pause and save program and output reply based on instruction below    \n        reply_instruction(\n            \"Tell the user about the latest advances in AI. Mention that \"\n            \"a new framework called AutoGRAMS was recently released that allows greater control over AI agents.\",\n            ADDRESS=\"tell_about_ai\"\n        )\n        #continue program here after user response\n\n    else:\n        #pause and save program and output reply based on instruction below  \n        reply_instruction(\n            \"Confirm with the user what they would prefer to talk about.\",\n            ADDRESS=\"ask_preference\"\n        )\n        #continue program here after user response\n\n    #infinite while loop continues conversation as long as user keeps responding\n    while True:\n        #pause and save program and output reply based on instruction below\n        reply_instruction(\"Respond to the user.\", ADDRESS=\"continue_conversation\")\n        #continue program here after user response\n</code></pre></p> <p>What's Happening Here? 1. <code>reply</code> and <code>reply_instruction</code>: These nodes prompt the user. They pause the program, send a reply, and wait for user input before resuming.</p> <ol> <li> <p><code>yes_or_no</code>: This function decides the next flow based on an AI model's interpretation of user input. It\u2019s not user-facing; instead, it processes the chatbot\u2019s own decision.</p> </li> <li> <p><code>ADDRESS</code> Parameters: These labels allow the chatbot to jump to specific points (Advanced), enabling dynamic flow and mid-conversation updates, and can also be used to help visualize the program</p> </li> </ol> <p>When the chatbot hits a reply or reply_instruction, it sends the response and pauses, saving the full state and memory of the program to a serializable object that can be saved and reloaded. After the user replies, the program resumes from that point, maintaining context.</p> <p>All of these nodes and function we are calling have access to a shared memory object, automatically logging the conversation and allowing prompts to be formed automatically, and managing the stack for each time we return.  </p>"},{"location":"#step-2","title":"Step 2","text":"<p>Next, create a script called <code>run_simple_example.py</code> to run the chatbot function. This script will use the <code>Autogram</code> class to manage memory and handle user interactions. <pre><code>from autograms import Autogram\nfrom simple_example import chatbot\n\n# Initialize the Autogram class with the chatbot function\nautogram = Autogram(root_function=chatbot)\n\n# Start the chatbot and capture the first reply\nchat_reply, memory_object = autogram.reply()\n\n# Loop to handle user interaction\nwhile True:\n    print(f\"Agent: {chat_reply}\")\n    user_input = input(\"User: \")\n\n    # Pass user input to the chatbot and get to the next pause point in our chatbot function (reply or reply_instruction)\n    chat_reply, memory_object = autogram.reply(user_input, memory_object=memory_object)\n</code></pre></p> <p>use <code>python run_simple_example.py</code> to run the chatbot. </p> <p>Alternatively, from the root directory of the autograms repository, you can run your chatbot with:</p> <pre><code>python run_autogram.py --autogram_file path/to/simple_example.py\n</code></pre> <p>You can also quickly debug common problems in the chatbot with </p> <pre><code>python run_debug_autogram.py --autogram_file path/to/simple_example.py\n</code></pre> <p>which will simulate many conversations with the user without calling any model APIs</p>"},{"location":"#step-3-visualize-the-code","title":"Step 3: visualize the code","text":"<p>create a new file called generate_graph.py <pre><code>from autograms.graph import compile_graph\nfrom simple_example import chatbot\n\n# Compile and save the graph for the chatbot\ngraph = compile_graph(chatbot)\ngraph.save_visualization(save_folder=\"simple_example_graph\", graph_format=\"png\")\n</code></pre></p> <p>and run <code>python generate_graph.py</code></p> <p>This will create two files:</p> <p>PNG Image: A static graph visualization. Interactive HTML graph: allows you to highlight nodes to see code.</p>"},{"location":"#more-examples-and-useful-functions","title":"More examples and useful functions","text":"<p>See <code>run_autogram.py</code>, <code>debug_augogram.py</code>, and <code>visualize_autogram.py</code> for code that is useful for running, debugging, and visualizing autograms. Also see the <code>/examples</code> folder for more examples of autograms chatbots with comments</p>"},{"location":"documentation/autogram/","title":"Autogram Class Documentation","text":"<p>The <code>Autogram</code> class provides a high-level interface for managing chatbot interactions using the Autograms system. It wraps around the root <code>AutogramsFunction</code> and it's respective module, and handles memory management, serialization, and interaction logic.</p>"},{"location":"documentation/autogram/#class-autogram","title":"Class: <code>Autogram</code>","text":"<p>Located in <code>autograms.functional.Autogram</code>.</p>"},{"location":"documentation/autogram/#description","title":"Description","text":"<p>This class acts as a wrapper around the root AutogramsFunction, enabling stateful chatbot management through memory handling, serialization, API integration, and reply generation. It provides a streamlined interface for building conversational agents with dynamic control flows.</p>"},{"location":"documentation/autogram/#methods","title":"Methods","text":""},{"location":"documentation/autogram/#__init__root_functionnone-autogram_confignone-api_keysnone-global_reload_typepersistent_only-test_modefalse","title":"<code>__init__(root_function=None, autogram_config=None, api_keys=None, global_reload_type=\"persistent_only\", test_mode=False)</code>","text":"<p>Description: Initializes the Autogram object with a root function, configuration, and API keys.</p> <p>Parameters: - <code>root_function (AutogramsFunction, optional)</code>: The root function defining the chatbot behavior. - <code>autogram_config (AutogramConfig, optional)</code>: Configuration object for the Autogram. - <code>api_keys (dict, optional)</code>: Dictionary of API keys, with an optional <code>\"load_from_env\"</code> to load keys from environment variables. - <code>test_mode (bool, optional)</code>: Enables test mode for automatic, mock responses without calling APIs.</p>"},{"location":"documentation/autogram/#use_memorymemory_objectnone-memory_dictnone","title":"<code>use_memory(memory_object=None, memory_dict=None)</code>","text":"<p>Description: Sets and manages memory for chatbot sessions or replies, ensuring proper cleanup.</p> <p>Parameters: - <code>memory_object (MemoryObject or SerializableMemory, optional)</code>: The memory object to use. Defaults to creating a new memory object. - <code>memory_dict (dict, optional)</code>: Dictionary containing serialized memory state.</p> <p>Yields: - <code>memory_object</code>: The active memory object for the session.</p>"},{"location":"documentation/autogram/#use_config","title":"<code>use_config()</code>","text":"<p>Description: Temporarily sets a simple memory configuration for isolated use. Used for testing or lightweight interactions.</p> <p>Yields: - <code>SimpleMemory</code>: An isolated memory object based on the current configuration.</p>"},{"location":"documentation/autogram/#replyuser_reply-memory_objectnone-memory_dictnone-kwargs","title":"<code>reply(user_reply=\"\", memory_object=None, memory_dict=None, **kwargs)</code>","text":"<p>Description: Generates a chatbot reply based on user input and memory. It automatically sets the user reply and manages the memory scope, and calls the root function. It also makes sure that (non-user-specific) globals in the module don't change by reloading them from a serialized state. </p> <p>Parameters: - <code>user_reply (str)</code>: The user's input message. - <code>memory_object (MemoryObject, optional)</code>: Memory object to use for the reply. - <code>memory_dict (dict, optional)</code>: Serialized memory to initialize a new memory object. - <code>**kwargs</code>: Additional arguments passed to the root function.</p> <p>Returns: - <code>tuple</code>: A reply message and the updated memory object.</p>"},{"location":"documentation/autogram/#applyselfuser_replynonememory_objectnonememory_dictnonefuncnonekwargs","title":"<code>apply(self,user_reply=None,memory_object=None,memory_dict=None,func=None,**kwargs)</code>","text":"<p>Description: Like reply it is an entry point into the chatbot, but it is general. It still has similar behavior to reply() by default, but it can be used to call other functions besides the root functions, including @autograms_external(). It also gives a more general AutogramReturn object which can include a reply, but can also include other data, or function returns if the called function returns normally without a reply. Uses cases include chatbots that need to return additional (and potentially multi-modal) outputs, or as an entry point to functions that modify the memory of the chatbot externally. </p> <p>Parameters: - <code>user_reply (str)</code>: The user's input message. - <code>memory_object (MemoryObject, optional)</code>: Memory object to use for the reply. - <code>memory_dict (dict, optional)</code>: Serialized memory to initialize a new memory object. - <code>func (AutogramsFunction,AutogramsExternal, or (normal python) function)</code> function to be called, defaults to <code>root_function</code> of autogram. - <code>**kwargs</code>: arguments passed to the called function</p> <p>Returns: - <code>result</code>: An <code>AutogramsReturn</code> object (<code>autograms.program_control.AutogramReturn</code>), which includes fields    --<code>reply</code> - either the reply to user or None if the function returned normally with out a reply    --<code>data</code> - any additional data that was sent back. For instance if the model is programed to reply but also send back another variable such as an image or other information.     --<code>func_return</code> - the returned value of the function if it returned normally, or None if it returned with a reply.</p>"},{"location":"documentation/autogram/#add_user_replyuser_replynone","title":"<code>add_user_reply(user_reply=None)</code>","text":"<p>Description: Adds a user reply to the current memory.</p> <p>Parameters: - <code>user_reply (str)</code>: The user's reply to be added to memory.</p> <p>Raises: - <code>Exception</code>: If no memory is set.</p>"},{"location":"documentation/autogram/#deserializedata-serialization_typepartial","title":"<code>deserialize(data, serialization_type=\"partial\")</code>","text":"<p>Description: Deserializes memory from a given data string.</p> <p>Parameters: - <code>data (str)</code>: Serialized memory data. - <code>serialization_type (str)</code>: Type of serialization:   - <code>\"partial\"</code>   - <code>\"full\"</code>   - <code>\"json\"</code> (not implemented)</p> <p>Returns: - <code>MemoryObject</code>: Deserialized memory object.</p>"},{"location":"documentation/autogram/#loadfile_path","title":"<code>load(file_path)</code>","text":"<p>Description: Loads memory from a file.</p> <p>Parameters: - <code>file_path (str)</code>: Path to the file containing serialized memory. - <code>serialization_type (str)</code>: Type of serialization:   - <code>\"partial\"</code>   - <code>\"full\"</code>   - <code>\"json\"</code> (not implemented)</p> <p>Returns: - <code>MemoryObject</code>: Loaded memory object.</p>"},{"location":"documentation/autogram/#savefile-memory_objectnone","title":"<code>save(file, memory_object=None)</code>","text":"<p>Description: Saves the current memory state to a file.</p> <p>Parameters: - <code>file (str)</code>: Path to the file where memory should be saved. - <code>memory_object (MemoryObject, optional)</code>: Memory object to save. Defaults to the current memory.</p>"},{"location":"documentation/autogram/#serializememory_objectnone-serialization_typepartial","title":"<code>serialize(memory_object=None, serialization_type=\"partial\")</code>","text":"<p>Description: Serializes the current memory state to a string.</p> <p>Parameters: - <code>memory_object (MemoryObject, optional)</code>: Memory object to serialize. Defaults to the current memory.</p> <p>Returns: - <code>str</code>: Serialized memory data.</p>"},{"location":"documentation/autograms_decorators/","title":"Autograms Decorators","text":""},{"location":"documentation/autograms_decorators/#autograms_function","title":"<code>@autograms_function()</code>","text":"<p>The <code>@autograms_function()</code> decorator is a core feature of the Autograms framework, designed to enable dynamic control flow in conversational agents. It transforms standard Python functions into resumable, stateful workflows that can pause, serialize, and resume seamlessly. This allows chatbots or other interactive agents to act as one continuously running program that can be paused and saved at any time while waiting for input.</p>"},{"location":"documentation/autograms_decorators/#key-features","title":"Key Features","text":"<ol> <li> <p>Pause and Resume Execution:    Functions decorated with <code>@autograms_function()</code> can pause at any point using special nodes (e.g., <code>reply</code>, <code>reply_instruction</code>). These pauses return temporary responses to the user, while the function's state is saved and can be resumed from the exact point it left off.</p> </li> <li> <p>Memory management:    The function\u2019s local variables, call stack, and conversation history are managed through an internal memory object, enabling serialization and reloading across different sessions.</p> </li> <li> <p>Conversation Scopes:    Manage how conversation history is retained using flexible scope options (<code>global</code>, <code>normal</code>, <code>local</code>).</p> </li> <li> <p>Dynamic Control Flow:    The decorator supports advanced control mechanisms like GOTO and RETURNTO, allowing the program to jump to specific points in the function based on predefined <code>ADDRESS</code> labels in functions decorated by <code>@autograms_node</code>.</p> </li> </ol>"},{"location":"documentation/autograms_decorators/#parameters","title":"Parameters","text":"<ul> <li>conv_scope (str, optional):   Defines the conversation scope. Defaults to <code>\"global\"</code>.</li> <li><code>\"global\"</code>: Turns persist and are accessible across all nested calls.</li> <li><code>\"normal\"</code>: Turns are accessible within the current function but do not persist after returning.</li> <li><code>\"local\"</code>: Turns are isolated to the current function call.</li> </ul>"},{"location":"documentation/autograms_decorators/#example-usage","title":"Example usage","text":"<pre><code>from autograms.nodes import reply, reply_instruction\nfrom autograms.functional import yes_or_no\nfrom autograms import autograms_function\n\n#this decorator allows the function to have special behavior such as temporarily returning replies\n@autograms_function()\ndef chatbot():\n\n    #model goes first here, we have a fixed introductory message. Pauses program and returns a reply\n    reply(\"Would you like me to tell you more about the latest advances in AI?\", ADDRESS=\"ask_question\")\n    #program will continue from this point after first user reply \n\n    #The agent decides whether it thinks the user wants to talk about AI based on their response  \n    user_wants_ai = yes_or_no(\"does the user want to talk about AI?\")\n\n    if user_wants_ai:\n        #pause and save program and return a reply based on instruction below    \n        reply_instruction(\n            \"Tell the user about the latest advances in AI. Mention that \"\n            \"a new framework called AutoGRAMS was recently released that allows greater control over AI agents.\",\n            ADDRESS=\"tell_about_ai\"\n        )\n        #continue program here after user response\n\n    else:\n         #pause and save program and return a reply based on instruction below  \n        reply_instruction(\n            \"Confirm with the user what they would prefer to talk about.\",\n            ADDRESS=\"ask_preference\"\n        )\n        #continue program here after user response\n\n    #infinite while loop continues conversation as long as user keeps responding\n    while True:\n        #pause and save program and output reply based on instruction below\n        reply_instruction(\"Respond to the user.\", ADDRESS=\"continue_conversation\")\n        #continue program here after user response\n</code></pre>"},{"location":"documentation/autograms_decorators/#restrictions","title":"Restrictions","text":"<p>While <code>@autograms_function</code> supports various control flow constructs, it may not handle resumption within certain node types (e.g., <code>try</code>-<code>except</code>, <code>finally</code>). Ensure that <code>ADDRESS</code> labels and pauses occur within supported contexts like loops and conditionals.</p>"},{"location":"documentation/autograms_decorators/#autograms_external","title":"<code>@autograms_external()</code>","text":"<p><code>@autograms_external()</code> is meant to allow for functions that interact with user specific memory of the program to be called from outside of the normal conversation chain. The are mainly used to interact with user specific global variables</p>"},{"location":"documentation/autograms_decorators/#example-use","title":"example use","text":"<pre><code>@autograms_external()\ndef update_user_settings(user_command):\n\n   thought(f\" The user made this request {user_command}. Should we change the chat_style setting? it is currently set to {user_globals['chat_style']}, and the options are {','.join(chat_styles)}\")\n\n   idx =multiple_choice(\"what should the chat style be set to?\",chat_styles) \n\n   user_globals['chat_style'] = chat_styles[idx]\n</code></pre> <p>And then in the Autogram object wrapping the module, you can do <pre><code>autogram.apply(memory_object = memory_object,function=update_user_settings,user_command=user_command)\n</code></pre></p> <p>@autograms_external functions do not permanently modify the stack in the memory_object, are mainly used to change the UserGlobals object for user specific global variables defined in the module if there is one. </p>"},{"location":"documentation/autograms_decorators/#autograms_node","title":"@autograms_node","text":"<p>Decorator for defining addressable nodes in Autograms. These nodes can serve as targets for <code>GOTO</code> and <code>RETURNTO</code> statements, enabling dynamic control flow within a function. They can also be used for logging and visualization purposes for showing graphs of the program, even if used without <code>GOTO</code> or <code>RETURNTO</code>.</p>"},{"location":"documentation/autograms_decorators/#behavior","title":"Behavior","text":"<ul> <li>Allows the function to be a target for <code>GOTO</code> and <code>RETURNTO</code> statements.</li> <li>When an <code>ADDRESS</code> is provided, the function call is logged and can be revisited.</li> </ul>"},{"location":"documentation/autograms_decorators/#example-usage_1","title":"Example usage","text":"<p>The following uses an autograms node combined with GOTO to design a chatbot that always replies with the next number of the Fibonacci sequence <pre><code>from autograms import autograms_node,autograms_function\nfrom autograms.functional import GOTO, reply\n\n@autograms_node\ndef reply_value(value,ADDRESS=None):\n    #pauses program to reply\n    reply(f\"value is now {value}\")\n\n@autograms_function():\ndef fibonacci():\n   x=1\n   reply_value(x)\n\n   x_last=x\n\n\n   reply_value(x+x_last,ADDRESS=\"output_x\")\n   x_last=x\n   x = x+x_last\n\n   #facilitates loop  by going to previous line with `reply_value(x+x_last,ADDRESS=\"output_x\")`\n   GOTO(destination=\"output_x\")\n</code></pre></p>"},{"location":"documentation/config/","title":"AutogramConfig Documentation","text":"<p><code>AutogramConfig</code> is the main configuration class in AutoGRAMS. It consolidates all the settings for model and prompt settings in AutoGRAMS. This reference explains every parameter, how the config interacts with other parts of AutoGRAMS, and how users can take advantage of it to customize their chatbot experience.</p>"},{"location":"documentation/config/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Class Definition</li> <li>Parameters</li> <li>Usage Patterns</li> <li>Overriding Config at Runtime</li> <li>Examples</li> </ul>"},{"location":"documentation/config/#overview","title":"Overview","text":"<p>In AutoGRAMS:</p> <ul> <li>Chatbot modules and functions usually do not hardcode model details or generation arguments.</li> <li> <p>An <code>AutogramConfig</code> object is passed when an <code>Autogram</code> is initialized. This dictates which model endpoints to call, max generation lengths, how many retries to attempt, how prompts are structured, etc.</p> </li> <li> <p>when using the <code>run_autogram.py</code> script to run your autogram, you can also pass in the config as a json file with the desired arguments, for example:</p> <ul> <li><code>run_autogram.py --autogram_file path/to/autogram/file.py --config_file path/to/config/file.json</code></li> </ul> </li> </ul>"},{"location":"documentation/config/#class-definition","title":"Class Definition","text":"<pre><code>class AutogramConfig():\n    def __init__(\n        self,\n        max_tokens=1024,\n        default_prompt=\"You are an agent.\",\n        agent_name=\"Agent\",\n        user_name=\"User\",\n        instruction_name=\"Instruction\",\n        chatbot_type=\"openai\",\n        chatbot_path=\"gpt-4o\",\n        system_prompt_in_turns=False,\n        error_response=None,\n        chatbot_generation_args=None,\n        chatbot_max_tries=3,\n        chatbot_wait_per_try=5,\n        chatbot_max_input_len=3500,\n        classifier_max_tries=2,\n        classifier_wait_per_try=5,\n        classifier_max_input_len=2048,\n        exclude_classifier_system_prompt=False,\n        banned_phrases=None,\n        post_process_response=True,\n        classifier_type=None,\n        classifier_path=None,\n        classifier_mode=\"json\",\n        embedding_type=None,\n        embedding_path=\"text-embedding-3-small\",\n        instruction_template=None,\n        reply_start_type=\"suffix\",\n        default_reply_start_template=None,\n        user_instruction_template=None,\n        default_question_prompt=None,\n        default_transition_context=1,\n        reply_suffix_inst_conversion=None,\n        chatbot_proxy_port=8080,\n        classifier_proxy_port=None,\n        embedding_proxy_port=None,\n        **kwargs\n    ):\n        ...\n</code></pre>"},{"location":"documentation/config/#parameters","title":"Parameters","text":"<p>Below is a comprehensive list of the parameters that define an <code>AutogramConfig</code>. Some parameters have defaults and might not need explicit changes unless you want to customize certain behavior.</p> Parameter Type Default Description max_tokens int <code>1024</code> Maximum response length in tokens. Similar to OpenAI\u2019s <code>max_tokens</code> or HuggingFace\u2019s token limit. default_prompt str <code>\"You are an agent.\"</code> Starting prompt for your chatbot or agent, used if no other system prompt is set. agent_name str <code>\"Agent\"</code> Name of the AI assistant, used in prompts and for generating structured replies. user_name str <code>\"User\"</code> Name of the user, used in conversation transcripts or for labeling user turns. instruction_name str <code>\"Instruction\"</code> Label for the instruction token in your conversation templates. Typically appended to the system prompt. chatbot_type str <code>\"openai\"</code> Specifies which underlying model or approach is used for the chatbot. Options might include <code>\"openai\"</code>, <code>\"proxy\"</code>, or <code>\"huggingface_tgi\"</code>. chatbot_path str <code>\"gpt-4o\"</code> Model path or endpoint for the chatbot. For <code>\"openai\"</code>, this might be a model name (e.g., <code>\"gpt-3.5-turbo\"</code>). For <code>\"proxy\"</code>, an endpoint or model identifier. system_prompt_in_turns bool <code>False</code> If <code>True</code>, the system prompt is embedded in the first user turn (for certain system behaviors). If <code>False</code>, it\u2019s sent as a dedicated system message. error_response str <code>None</code> \u2192 fallback to internal The fallback response if the chatbot encounters an error. If <code>None</code>, uses a default message. chatbot_generation_args dict <code>{\"temperature\": 0.7}</code> A dictionary of generation parameters akin to OpenAI or HF model arguments. E.g., <code>{\"temperature\":0.8,\"top_p\":0.9}</code>. chatbot_max_tries int <code>3</code> Maximum number of retries if the chatbot request fails. chatbot_wait_per_try int <code>5</code> Number of seconds to wait between retries for chatbot requests. chatbot_max_input_len int <code>3500</code> Maximum allowable input length in tokens for the chatbot prompt. classifier_max_tries int <code>2</code> Maximum number of retries for classification requests (e.g., yes/no or multiple-choice logic). classifier_wait_per_try int <code>5</code> Wait time between retries for classifier requests. classifier_max_input_len int <code>2048</code> Token limit for classifier prompts. exclude_classifier_system_prompt bool <code>False</code> If <code>True</code>, the system prompt is not appended for classification. Useful if you want classifier logic to be minimal. banned_phrases list[str] <code>None</code> \u2192 fallback to a default A list of phrases the chatbot should avoid. If the chatbot produces any of these, a retry or a filtered response may be triggered. post_process_response bool <code>True</code> If <code>True</code>, the agent\u2019s final response is post-processed (e.g., removing certain artifacts). classifier_type str <code>None</code> \u2192 fallback to <code>chatbot_type</code> The model type for classifier logic. If <code>None</code>, defaults to <code>chatbot_type</code>. Examples: <code>\"openai\"</code>, <code>\"huggingface_tgi\"</code>, or <code>\"proxy\"</code>. classifier_path str <code>None</code> \u2192 fallback to <code>chatbot_path</code> The model path or endpoint for classifier. If <code>None</code>, defaults to <code>chatbot_path</code>. classifier_mode str <code>\"json\"</code> How classification is performed: <code>\"logit\"</code> or <code>\"json\"</code>. If <code>\"logit\"</code>, the model must support logit bias or special classification logic. embedding_type str <code>None</code> \u2192 <code>\"proxy\"/\"openai\"</code> logic Embedding model type, e.g., <code>\"openai\"</code>, <code>\"proxy\"</code>. If <code>None</code> and <code>chatbot_type == \"proxy\"</code>, we set <code>\"proxy\"</code>, else <code>\"openai\"</code>. embedding_path str <code>\"text-embedding-3-small\"</code> Model path/endpoint for the embedding model. instruction_template str <code>None</code> \u2192 uses a default Template for how the instruction and user response are combined. reply_start_type str <code>\"suffix\"</code> How to format reply starts in the conversation. Must be one of <code>\"suffix\"</code>, <code>\"prefix\"</code>, or <code>\"none\"</code>. default_reply_start_template str Various default The base template for reply starts. user_instruction_template str <code>None</code> Template for user instructions if you simulate user messages. default_question_prompt str <code>None</code> \u2192 fallback to default The default prompt text used for multiple-choice or yes/no questions. default_transition_context int <code>1</code> Number of conversation turns to include for context transitions. reply_suffix_inst_conversion str <code>None</code> \u2192 fallback Template for instructing the chatbot how to handle <code>reply_suffix</code> nodes. chatbot_proxy_port int <code>8080</code> Port for chatbot proxies. classifier_proxy_port int <code>None</code> \u2192 fallback to <code>chatbot_proxy_port</code> Port for classifier proxies if different from chatbot. embedding_proxy_port int <code>None</code> \u2192 fallback to <code>chatbot_proxy_port</code> Port for embedding proxies if different. **kwargs dict N/A Additional arguments for deprecated fields. Raises error if not recognized."},{"location":"documentation/config/#usage-patterns","title":"Usage Patterns","text":"<ol> <li> <p>Creating a Default Config <pre><code>config = AutogramConfig()\n# uses GPT-4o with temperature=0.7 (OpenAI style), 1024 max_tokens, etc.\n</code></pre></p> </li> <li> <p>Customizing </p> </li> </ol> <pre><code>config = AutogramConfig(\n    chatbot_type=\"proxy\",\n    chatbot_path=\"http://localhost:8080/v1\",\n    max_tokens=1500,\n    chatbot_generation_args={\"temperature\": 0.5},\n)\n</code></pre>"},{"location":"documentation/config/#overriding-config-at-runtime","title":"Overriding Config at Runtime","text":"<ul> <li>If do you want to override or hardcode certain behaviors (e.g., temperature, max_tokens, or a specific model path), you can do so in the config or by passing openai style arguments to model calling functions in <code>autograms.functional</code> <ul> <li>for example     <pre><code>reply_instruction(instruction=\"reply to the user\",max_tokens=2048).\n</code></pre></li> <li>if you want to reuse certain hardcoded behaviors that over ride the config, you could do something like     <pre><code>model1_settings= {\"model\":\"gpt-3.5-turbo\",\"max_tokens\":2048}\nreply_instruction(instruction=\"reply to the user and ask a question\",**model1_settings).\nreply_instruction(instruction=\"reply to the user with a follow up question\",**model1_settings).\n</code></pre></li> </ul> </li> <li>overriding max_tokens or other generation args for certain responses can be useful for specific <code>thought()</code> or <code>reply_instruction()</code> calls. However, overwriting the \"model\" argument probably isn't necessary for most applications, unless you want to use completely different models for different calls. The config already lets you set different models for LLM-based classification, generation, and embeddings, so often its best practice to set the model names in the config so they can be more easily changed and the autogram module can be model agnostic.</li> </ul>"},{"location":"documentation/control_flow_functions/","title":"Control Flow functions","text":""},{"location":"documentation/control_flow_functions/#control-flow-functions","title":"Control Flow Functions","text":"<p>Control flow functions enable dynamic navigation within the Autograms chatbot and are meant to be used within @autograms_functions in order to jump to @autograms_nodes with predefined addresses. They manage transitions between different points in a conversation or program logic. They are not strictly necessary since careful application of loops, conditionals, and variables should be able to achieve any arbitrary graph, however they do make complex stateful chatbots easier to implement with the desired graph logic. GOTO statements are not normally allowed in python, but are possible in @autograms_functions using a combination of special exceptions and dynamic abstract syntax tree manipulation.</p>"},{"location":"documentation/control_flow_functions/#transitiontransition_question-transitions-max_turns1-kwargs","title":"TRANSITION(transition_question, transitions, max_turns=1, kwargs)**","text":"<p><code>autograms.functional.TRANSTION</code> Manages a transition based on multiple-choice input.</p> <ul> <li>Parameters:</li> <li><code>transition_question</code> (str): The question prompting the transition.</li> <li><code>transitions</code> (dict): Maps choices to their corresponding addresses.</li> <li><code>max_turns</code> (int): Maximum attempts for a valid response.</li> <li> <p><code>**kwargs</code>: Additional arguments for the model.</p> </li> <li> <p>Raises:</p> </li> <li><code>GoTo</code>: To jump to the target address.</li> </ul>"},{"location":"documentation/control_flow_functions/#exitdata","title":"EXIT(data={})","text":"<p><code>autograms.functional.EXIT</code> Exits the current function and returns the provided data.</p> <ul> <li>Parameters:</li> <li> <p><code>data</code> (dict): Data to be returned.</p> </li> <li> <p>Raises:</p> </li> <li><code>FunctionExit</code>: Used to exit the current function.</li> </ul>"},{"location":"documentation/control_flow_functions/#gotodestination","title":"GOTO(destination)","text":"<p><code>autograms.functional.GOTO</code> Jumps to a specific address within the function.</p> <ul> <li>Parameters:</li> <li> <p><code>destination</code> (str): Target address for the jump.</p> </li> <li> <p>Raises:</p> </li> <li><code>GoTo</code>: Used to jump to the specified address.</li> </ul>"},{"location":"documentation/control_flow_functions/#returntodestination","title":"RETURNTO(destination)","text":"<p><code>autograms.functional.RETURNTO</code> Returns to an address within an earlier function in the call stack.</p> <ul> <li>Parameters:</li> <li> <p><code>destination</code> (str): Target address for returning.</p> </li> <li> <p>Raises:</p> </li> <li><code>ReturnTo</code>: Used to return to the specified address.</li> </ul>"},{"location":"documentation/decision_functions/","title":"Decision functions","text":""},{"location":"documentation/decision_functions/#decision-functions","title":"Decision Functions","text":"<p>Decision functions are used to make choices within the Autograms chatbot. Depending on whether <code>classifier_mode</code> in the AutogramConfig is set to <code>json</code> or <code>logit</code>, they either use structured outputs or a restricted set of logits to force an LLM to chose from a limited set of choices. This enables an LLM to determine branch points in the program. </p>"},{"location":"documentation/decision_functions/#yes_or_noquestion-max_turns1-kwargs","title":"yes_or_no(question, max_turns=1, <code>**kwargs</code>)","text":"<p><code>autograms.functional.yes_or_no</code> Asks the agent a yes-or-no question.</p> <ul> <li>Parameters:</li> <li><code>question</code> (str): The yes-or-no question.</li> <li><code>max_turns</code> (int): Maximum attempts for a valid response.</li> <li> <p><code>**kwargs</code>: Additional arguments for openai style model request (can override model or generation args set in config). Also allows <code>multi_modal_inputs</code> keyword to pass a list of openai message style dictionaries with image necodings, image url, or audio (see call_classifier).</p> </li> <li> <p>Returns:</p> </li> <li><code>bool</code>: <code>True</code> for \"Yes\", <code>False</code> for \"No\".</li> </ul>"},{"location":"documentation/decision_functions/#multiple_choicequestion-choices-max_turns1-kwargs","title":"multiple_choice(question, choices, max_turns=1, <code>**kwargs</code>)","text":"<p><code>autograms.functional.multiple_choice</code> Presents the agent with a multiple-choice question.</p> <ul> <li>Parameters:</li> <li><code>question</code> (str): The question to ask.</li> <li><code>choices</code> (list[str]): List of possible choices.</li> <li><code>max_turns</code> (int): Maximum attempts for a valid response.</li> <li> <p><code>**kwargs</code>: Additional arguments for openai style model request (can override model or generation args set in config).  Also allows <code>multi_modal_inputs</code> keyword to pass a list of openai message style dictionaries with image necodings, image url, or audio (see call_classifier).</p> </li> <li> <p>Returns:</p> </li> <li><code>int</code>: Index of the selected choice.</li> </ul>"},{"location":"documentation/decision_functions/#multiple_choice_logitsquestionchoicesmax_turns1kwargs","title":"multiple_choice_logits(question,choices,max_turns=1,<code>**kwargs</code>)","text":"<p><code>autograms.functional.multiple_choice_logits</code>     - <code>question</code> (str): The multiple-choice question.     - <code>choices</code> (list of str): Available choices.     - <code>max_turns</code> (int): Maximum number of turns for decision making.     - <code>**kwargs</code>: Additional arguments for openai style model request (can override model or generation args set in config). Also allows <code>multi_modal_inputs</code> keyword to pass a list of openai message style dictionaries with image encodings, image url, or audio (see call_classifier).</p> <ul> <li>Parameters:</li> </ul>"},{"location":"documentation/decision_functions/#decision_chaininstructionchainkwargs","title":"decision_chain(instruction,chain,<code>**kwargs</code>)","text":"<p><code>autograms.functional.decision_chain</code> Execute a series of multiple choice style decisions with one call to the model where each thought can have a different prompt. Uses structured outputs to force a sequence of constrained outputs. - Parameters:</p> <ul> <li>instruction (str): Instruction for the model</li> <li>chain list[str]: list of dictionaries, each each dictionary needs to have fields 'question' and 'choices', where question is a string and choices is a list of strings corresponding to multiple choice answers to the question.list of prompts for each thought</li> <li>Returns:     output_list (list[str]): Gives the model generation for each prompt</li> </ul>"},{"location":"documentation/embeddings/","title":"Embedding functions","text":""},{"location":"documentation/embeddings/#embedding-functions","title":"Embedding Functions","text":"<p>Embedding functions generate vector representations of text, which is most often used for retrieval and memory within the chatbot.</p>"},{"location":"documentation/embeddings/#get_single_embeddingtext-kwargs","title":"get_single_embedding(text, <code>**kwargs</code>)","text":"<p><code>autograms.functional.get_single_embedding</code> Fetches a single embedding for the given text.</p> <ul> <li>Parameters:</li> <li><code>text</code> (str): The input text to generate an embedding for.</li> <li> <p><code>model_type</code> (str, default='openai'): The type of model to use for embedding.</p> </li> <li> <p>Returns:</p> </li> <li><code>list[float]</code>: A vector representation of the input text.</li> </ul>"},{"location":"documentation/embeddings/#get_batch_embeddingstextskwargs","title":"get_batch_embeddings(texts,<code>**kwargs</code>)","text":"<p><code>autograms.functional.get_batch_embeddings</code> Fetches embeddings for a batch of texts.</p> <ul> <li>Parameters:</li> <li><code>texts</code> (list[str]): A list of input texts.</li> <li> <p><code>model_type</code> (str, default='openai'): The type of model to use for embeddings.</p> </li> <li> <p>Returns:</p> </li> <li><code>list[list[float]]</code>: A list of vector representations for each input text.</li> </ul>"},{"location":"documentation/helper/","title":"Utility Functions Documentation","text":"<p>This section documents utility functions in <code>autograms.functional</code> that provide additional functionality for code handling, execution, and parallel processing.</p>"},{"location":"documentation/helper/#extract_codeinput_string-code_typepython","title":"extract_code(input_string, code_type='python')","text":"<p><code>autograms.functional.extract_code</code></p> <p>Description: Extracts and concatenates code blocks of a specified type (e.g., Python, JavaScript) from the input string.</p> <p>Parameters: - <code>input_string (str)</code>: The input string containing text and code blocks. - <code>code_type (str)</code>: The type of code to extract (e.g., <code>'python'</code>, <code>'javascript'</code>). Defaults to <code>'python'</code>.</p> <p>Returns: - <code>str</code>: The concatenated code from all specified code blocks, separated by newlines.</p>"},{"location":"documentation/helper/#execute_codecode-commandfirejail-noprofile-quiet-read-onlyhome-read-onlyusr-python3-timeout60-code_suffixpy","title":"execute_code(code, command=\"firejail --noprofile --quiet --read-only=/home --read-only=/usr python3\", timeout=60, code_suffix=\".py\")","text":"<p><code>autograms.functional.execute_code</code></p> <p>Description: Executes code in a sandboxed environment using a specified command, such as Firejail.</p> <p>Parameters: - <code>code (str)</code>: The code to execute. - <code>command (str)</code>: The command to execute the code, wrapped in a sandbox. Defaults to Firejail. - <code>timeout (int)</code>: Maximum execution time in seconds before timing out. Defaults to <code>60</code>. - <code>code_suffix (str)</code>: File extension for the temporary code file. Defaults to <code>\".py\"</code>.</p> <p>Returns: - <code>tuple</code>: A tuple containing:   - <code>message (str)</code>: Output messages, including <code>stdout</code>, <code>stderr</code>, or timeout messages.   - <code>success (bool)</code>: <code>True</code> if execution completed successfully, <code>False</code> otherwise.</p>"},{"location":"documentation/helper/#parallel_wrapperfunction-arg_list-mem_listnone-with_autograms_memorytrue","title":"parallel_wrapper(function, arg_list, mem_list=None, with_autograms_memory=True)","text":"<p><code>autograms.functional.parallel_wrapper</code></p> <p>Description: Runs a function with arguments in parallel using multiple processes. It initializes thread-local memory for each process and returns results along with the final thread-local state.</p> <p>Parameters: - <code>function (callable)</code>: The function to execute in parallel. - <code>arg_list (list)</code>: A list of argument dictionaries for each function call. - <code>mem_list (list, optional)</code>: A list of serialized memory objects for each task. Must match the length of <code>arg_list</code> if provided. Defaults to <code>None</code>. - <code>with_autograms_memory (bool)</code>: If <code>True</code>, thread-local memory is shared with the parallel processes. Defaults to <code>True</code>.</p> <p>Returns: - <code>list</code>: A list of function results.</p> <p>Raises: - <code>ValueError</code>: If <code>mem_list</code> is provided but its length does not match <code>arg_list</code>. - <code>Exception</code>: If an error occurs during parallel execution.</p>"},{"location":"documentation/logging_functions/","title":"Logging functions","text":""},{"location":"documentation/logging_functions/#logging-functions","title":"Logging Functions","text":"<p>Logging functions are responsible for recording conversation turns and internal reasoning turns in the Autograms chatbot. They are used to save information that helps form prompts.</p>"},{"location":"documentation/logging_functions/#log_chat_turnreply-instructionnone-line_numbernone-function_namenone","title":"log_chat_turn(reply, instruction=None, line_number=None, function_name=None)","text":"<p><code>autograms.functional.log_chat_turn</code> Logs a user-visible chat turn in the memory object.</p> <ul> <li>Parameters:</li> <li><code>reply</code> (str): The agent's reply.</li> <li><code>instruction</code> (str, optional): Instruction context for the reply.</li> <li><code>line_number</code> (int, optional): Line number of the reply.</li> <li><code>function_name</code> (str, optional): Function name where the reply was generated.</li> </ul>"},{"location":"documentation/logging_functions/#log_thought_turnreply-instruction","title":"log_thought_turn(reply, instruction)","text":"<p><code>autograms.functional.log_thought_turn</code> Logs a non-visible \"thought\" turn in the memory object.</p> <ul> <li>Parameters:</li> <li><code>reply</code> (str): The agent's internal thought response.</li> <li><code>instruction</code> (str): Instruction context for the thought.</li> </ul>"},{"location":"documentation/memory/","title":"Memory Module Documentation","text":"<p>In the AutoGRAMS framework, chatbots act like continually running programs, which means that the program's memory represents a memory of a conversation with a user. This section documents the key classes, functions, and patterns used in AutoGRAMS memory management. the main class that represents a memory of a specific instance of the program (conversation) is <code>MemoryObject</code>. Instances of memory_object have a <code>memory_dict</code> attribute that represents the entire state of the chatbots program, allowing you to load and save the exact state of autograms chatbots in a way that can be serialized to the disk. This enables you to easily switch between users that are active in conversations--you load the program memory for that user, continue executing the instance of the program for that user's conversation, and then save the memory when you pause or give a reply.</p>"},{"location":"documentation/memory/#understanding-globals","title":"Understanding Globals","text":"<ul> <li> <p>Module-Level Globals: Typically constants or system-wide resources, shared by all users. These are not managed by the memory object since they are not user specific. and are reinitialized every time you reload the module in python. You can define these as you normally would in python. After the Autogram class is initialized, module globals are serialized so they can't accidentally be changed in an interaction with a specific user.</p> </li> <li> <p>User-Specific Globals: Tied to a specific user\u2019s memory object, so each user\u2019s conversation state remains isolated.  </p> </li> <li>Initialized via <code>init_user_globals</code>.  </li> <li>Accessed through a <code>UserGlobals</code> instance.</li> </ul>"},{"location":"documentation/memory/#thread-safety","title":"Thread Safety","text":"<p>AutoGRAMS relies on context vars storage to ensure each thread has the correct <code>MemoryObject</code>. In multi-user or multi-thread scenarios, each user\u2019s memory context is separate and does not overwrite other users\u2019 states. </p>"},{"location":"documentation/memory/#use_memorymemory_object","title":"<code>use_memory(memory_object)</code>","text":"<p>A context manager for setting the memory in context vars storage within a <code>with</code> block. It ensures that the specified <code>memory_object</code> is active for the duration of the block and reset afterward.</p>"},{"location":"documentation/memory/#usage-example","title":"Usage Example","text":"<pre><code>with use_memory(some_memory_object):\n    result = chatbot()\n</code></pre> <ul> <li>Where <code>chatbot</code> is an AutoGRAMS function decorated with <code>@autograms_function</code>.  </li> <li>Purpose: Temporarily set the memory context for the thread, so any calls to <code>get_memory()</code> within the block return <code>some_memory_object</code>.</li> </ul>"},{"location":"documentation/memory/#get_memory","title":"<code>get_memory()</code>","text":"<p>Retrieves the current memory object from thread-local storage.</p> <ul> <li>Returns:  </li> <li>A <code>SerializableMemory</code> (or subclass) instance if active in this thread.  </li> <li><code>None</code> if no memory context is set.</li> </ul> <p>This function is widely used within AutoGRAMS functions (<code>@autograms_function</code>) to retrieve and manipulate the current conversation and stack state.</p>"},{"location":"documentation/memory/#memoryobject","title":"<code>MemoryObject</code>","text":"<p>The main memory class for managing chatbot applications, extending <code>SerializableMemory</code> with:</p> <ul> <li>Chatbot-Specific Functionality: Tracking conversation turns, stack frames, etc.  </li> <li>Serialization: Enough data to recreate the state after giving/replying.</li> </ul>"},{"location":"documentation/memory/#__init__","title":"<code>__init__(...)</code>","text":"<p>Initializes a <code>MemoryObject</code>.</p> <ul> <li>Parameters:  </li> <li><code>config</code> (object): Configuration object (e.g., <code>AutogramConfig</code>) for memory.  </li> <li><code>root_function</code> (function, optional): Root function for this memory context.  </li> <li><code>memory_dict</code> (dict, optional): Initial dictionary for storing memory-related data.</li> </ul>"},{"location":"documentation/memory/#add_user_replyuser_reply","title":"<code>add_user_reply(user_reply)</code>","text":"<p>Logs a user\u2019s reply in the conversation. This typically happens automatically if you pass a <code>user_reply</code> argument to <code>autogram.reply</code> or <code>autogram.apply</code>.</p> <ul> <li>Parameters:  </li> <li><code>user_reply</code> (str): The user\u2019s latest reply or message.</li> </ul>"},{"location":"documentation/memory/#serializeserialization_type","title":"<code>serialize(serialization_type)</code>","text":"<p>Serializes the memory object for storage or transmission.</p> <ul> <li>Parameters:  </li> <li><code>serialization_type</code> (str): The serialization mode, such as <code>\"full\"</code>, <code>\"partial\"</code>, or <code>\"json\"</code>.</li> <li>Returns: <code>str</code>   A serialized representation of this memory.</li> </ul>"},{"location":"documentation/memory/#savefile_name-serialization_type","title":"<code>save(file_name, serialization_type)</code>","text":"<p>Saves the memory object to disk.</p> <ul> <li>Parameters:  </li> <li><code>file_name</code> (str): Filename/path for storing the memory.  </li> <li><code>serialization_type</code> (str): Mode of serialization, e.g., <code>\"full\"</code>, <code>\"partial\"</code>, <code>\"json\"</code>.</li> </ul>"},{"location":"documentation/memory/#simplememory","title":"<code>SimpleMemory</code>","text":"<p>A lightweight memory class for managing prompts in simpler chatbot scenarios, without full serialization logic.</p>"},{"location":"documentation/memory/#__init___1","title":"<code>__init__(...)</code>","text":"<p>Creates a <code>SimpleMemory</code> instance:</p> <ul> <li>Parameters:  </li> <li><code>config</code> (object): Configuration object.  </li> <li><code>memory_dict</code> (dict, optional): Optional initial dictionary for memory.</li> </ul>"},{"location":"documentation/memory/#add_user_replyuser_reply_1","title":"<code>add_user_reply(user_reply)</code>","text":"<p>Logs a user\u2019s reply in this simple memory context.</p> <ul> <li>Parameters:  </li> <li><code>user_reply</code> (str): The user\u2019s message.</li> </ul>"},{"location":"documentation/memory/#init_user_globals","title":"<code>init_user_globals()</code>","text":"<p>Initializes a user-specific globals dictionary for the current Python module, ensuring each module can only create one set of user globals:</p> <ul> <li>Usage:   <pre><code>user_globals = init_user_globals()\nuser_globals[\"my_variable\"] = 42\n</code></pre></li> <li>Thread/Memory Link:   By using <code>UserGlobals</code>, any changes to <code>user_globals[...]</code> become tied to the user\u2019s memory context (the current <code>MemoryObject</code>).  </li> <li>Error Conditions:  </li> <li>Throws an exception if user globals were already initialized in this module. This prevents accidental re-initialization.</li> </ul>"},{"location":"documentation/memory/#behavior-and-rationale","title":"Behavior and Rationale","text":"<ul> <li>Module-Specific: <code>init_user_globals()</code> inspects the calling module, creates a unique ID, and ensures only one <code>UserGlobals</code> instance per module.  </li> <li>User-Specific:   Values in <code>UserGlobals</code> are stored under the current user\u2019s memory context. If a different user\u2019s memory is set (via <code>use_memory</code>), they see a different dictionary.</li> </ul>"},{"location":"documentation/memory/#userglobals","title":"<code>UserGlobals</code>","text":"<p>A special dictionary-like container for user-specific globals, ensuring each user\u2019s memory remains isolated. If the memory is <code>None</code>, it falls back to a local <code>init_dict</code>.</p>"},{"location":"documentation/memory/#implementation-details","title":"Implementation Details","text":"<ul> <li>Thread/Memory Linking:</li> <li> <p>Internally, <code>UserGlobals</code> calls <code>get_memory()</code> to find the active <code>MemoryObject</code> in this thread.  </p> </li> <li> <p>Isolation:</p> </li> <li>Each user (thread with a distinct memory) sees a different dictionary under <code>memory.memory_dict['user_globals']</code>.</li> </ul>"},{"location":"documentation/memory/#example-using-userglobals","title":"Example: Using <code>UserGlobals</code>","text":"<p>Below is a typical usage pattern:</p> <pre><code>user_globals = init_user_globals()\n\n# This initializes user_globals['name'] to None for all new users\nuser_globals[\"name\"] = None\n\n\n@autograms_function()\ndef chatbot():\n\n    reply(\"Hi, can you tell me your name?\")\n\n    if yes_or_no(\"did the user provide their name?\"):\n      #note that this will only update the name for the active memory object, allowing it to be a user/instance specific variable\n      user_globals[\"name\"] =thought_silent(\"Write down the user's name and nothing else\") \n</code></pre> <ul> <li>Explanation:  </li> <li><code>init_user_globals()</code> returns a <code>UserGlobals</code> instance, storing data in the user\u2019s memory.  </li> </ul>"},{"location":"documentation/memory/#additional-notes","title":"Additional Notes","text":"<ul> <li><code>create_module_id(module_globals)</code>: A helper function that generates a unique module ID from the file path or name, used internally by <code>init_user_globals()</code>.</li> <li>Thread Safety:  </li> <li>Each user\u2019s memory is separate via <code>use_memory(...)</code>, so concurrent calls in different threads do not overlap user-specific globals.  </li> <li>If no memory is set, <code>UserGlobals</code> uses a local <code>init_dict</code>.</li> </ul>"},{"location":"documentation/memory/#summary","title":"Summary","text":"<ul> <li><code>init_user_globals()</code>: Initializes the user-specific global dictionary once per module, throwing an error if re-initialized.  </li> <li><code>UserGlobals</code>: A dictionary-like interface for storing and retrieving user-specific variables, tied to the current memory context.  </li> <li>Isolation: Ensures each user\u2019s data is kept separate and persists only within their memory scope.  </li> <li>Integration: Works seamlessly with <code>use_memory(...)</code> and <code>@autograms_function</code> to maintain consistent state across multi-user or single-user applications.</li> </ul>"},{"location":"documentation/model_functions/","title":"Models calling functions","text":""},{"location":"documentation/model_functions/#model-calling-functions","title":"Model-Calling Functions","text":"<p>Model calling functions interface with external conversational or classification models. They prepare the input, send it to the model, and handle the response.</p>"},{"location":"documentation/model_functions/#call_conv_modelinstruction","title":"call_conv_model(instruction)","text":"<p><code>autograms.functional.call_conv_model</code> Calls the conversational model with the specified instruction.</p> <ul> <li>Parameters:</li> <li> <p><code>instruction</code> (str): Instruction for the conversational model.</p> </li> <li> <p>Returns:</p> </li> <li><code>str</code>: The model's response.</li> </ul>"},{"location":"documentation/model_functions/#call_classifierinput_str-answer_choices-model_typenone-modelnone-kwargs","title":"call_classifier(input_str, answer_choices, model_type=None, model=None, <code>**kwargs</code>)","text":"<p><code>autograms.functional.call_classifier</code> Calls a classification model to determine the best choice.</p> <ul> <li>Parameters:</li> <li><code>input_str</code> (str): The input text for classification.</li> <li><code>answer_choices</code> (list[str]): Possible answers.</li> <li><code>model_type</code> (str, optional): Type of the model.</li> <li><code>model</code> (str, optional): Path to the model.</li> <li><code>multi_modal_inputs</code> (list[dict], optional) --list of open ai style image or audio inputs for the model, for example :                     <pre><code>{\n\"type\": \"image_url\",\n\"image_url\": {\n    \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"}\n} \n</code></pre></li> <li> <p><code>**kwargs</code>: Additional model arguments.</p> </li> <li> <p>Returns:</p> </li> <li><code>tuple</code>: The chosen answer and a success flag.</li> </ul>"},{"location":"documentation/model_functions/#call_modelinput_turns-output_turns-system_prompt-system_prompt_in_turnsfalse-model_typenone-modelnone-kwargs","title":"call_model(input_turns, output_turns, system_prompt, system_prompt_in_turns=False, model_type=None, model=None, <code>**kwargs</code>)","text":"<p><code>autograms.functional.call_model</code> Calls a conversational model and retrieves a response.</p> <ul> <li>Parameters:</li> <li><code>input_turns</code> (list): The conversation input turns.</li> <li><code>output_turns</code> (list): The conversation output turns.</li> <li><code>system_prompt</code> (str): System prompt to guide the model.</li> <li><code>system_prompt_in_turns</code> (bool, default=False): Whether the system prompt is included in the turns.</li> <li><code>model_type</code> (str, optional): The type of model to call.</li> <li><code>model</code> (str, optional): Path to a specific model.</li> <li><code>multi_modal_inputs</code> (list[dict], optional) --list of open ai style image or audio inputs for the model, for example :                     <pre><code>{\n\"type\": \"image_url\",\n\"image_url\": {\n    \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"}\n} \n</code></pre></li> <li> <p><code>**kwargs</code>: Additional arguments for the model.</p> </li> <li> <p>Returns:</p> </li> <li><code>tuple</code>: A tuple containing:<ul> <li><code>result</code> (str): The model response.</li> <li><code>success</code> (bool): Whether the model call was successful.</li> </ul> </li> </ul>"},{"location":"documentation/model_functions/#call_object_modelinput_turns-output_turns-system_prompt-system_prompt_in_turnsfalse-model_typenone-modelnone-obj_structurenone-kwargs","title":"call_object_model(input_turns, output_turns, system_prompt, system_prompt_in_turns=False, model_type=None, model=None, obj_structure=None, <code>**kwargs</code>)","text":"<p><code>autograms.functional.call_object_model</code> Calls a model to generate a structured object based on input.</p> <ul> <li>Parameters:</li> <li><code>input_turns</code> (list[dict]): User input history.</li> <li><code>output_turns</code> (list[dict]): Model output history.</li> <li><code>system_prompt</code> (str): The system prompt.</li> <li><code>system_prompt_in_turns</code> (bool): Include system prompt in turns.</li> <li><code>model_type</code> (str, optional): Model type.</li> <li><code>model</code> (str, optional): Model path.</li> <li><code>obj_structure</code> (BaseModel): Pydantic model structure or json schema.</li> <li><code>multi_modal_inputs</code> (list[dict], optional) --list of open ai style image or audio inputs for the model, for example :                   <pre><code>{\n\"type\": \"image_url\",\n\"image_url\": {\n    \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"}\n} \n</code></pre></li> <li> <p><code>**kwargs</code>: Additional arguments.</p> </li> <li> <p>Returns:</p> </li> <li><code>BaseModel</code>: Generated object.</li> </ul>"},{"location":"documentation/nodes/","title":"Nodes","text":""},{"location":"documentation/nodes/#replytext-data-address","title":"reply(text, data, ADDRESS)","text":"<p><code>autograms.nodes.reply_suffix</code></p> <p>Autograms node wrapper for <code>autograms.functional.reply</code></p> <p>Raises a <code>ReplyExit</code> exception to send a direct reply.</p> <ul> <li>Parameters:</li> <li><code>text</code> (str): The reply message to the user.</li> <li><code>data</code> (dict, optional): optional data to be sent back in autograms return object sent by autogram.apply()</li> <li> <p><code>ADDRESS</code> (str) Only used by compiler--must be string literal or predefined constant. Used to recover position of function call in the code of an @autograms_function. Allows for GOTO and RETURNTO to jump to location in the code.</p> </li> <li> <p>Raises:</p> </li> <li><code>ReplyExit</code>: Used to exit the current function and send the reply.</li> </ul>"},{"location":"documentation/nodes/#reply_suffixinstruction-data-address","title":"reply_suffix(instruction, data, ADDRESS)","text":"<p><code>autograms.nodes.reply_suffix</code></p> <p>Autograms node wrapper for <code>autograms.functional.reply_suffix</code></p> <p>Replies to the user and includes the specified suffix in the response.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): The suffix to be included in the reply.</li> <li><code>data</code> (dict, optional): optional data to be sent back in autograms return object sent by autogram.apply()</li> <li><code>ADDRESS</code> (str, optional) Only used by compiler--must be string literal or predefined constant. Used to recover position of function call in the code of an @autograms_function. </li> <li>Raises:</li> <li><code>ReplyExit</code>: Used to exit the current function and send the reply.</li> </ul>"},{"location":"documentation/nodes/#reply_instructioninstruction-data-address","title":"reply_instruction(instruction, data,  ADDRESS)","text":"<p><code>autograms.nodes.reply_instruction</code></p> <p>Autograms node wrapper for <code>autograms.functional.reply_instruction</code> Node wrapper for autograms.functional.reply_instruction</p> <p>Replies based on an instruction generated by the conversational model.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): The instruction to guide the reply.</li> <li><code>data</code> (dict, optional): optional data to be sent back in autograms return object sent by autogram.apply()</li> <li><code>ADDRESS</code> (str, optional) Only used by compiler--must be string literal or predefined constant. Used to recover position of function call in the code of an @autograms_function. Allows for GOTO and RETURNTO to jump to location in the code.</li> <li>Raises:</li> <li><code>ReplyExit</code>: Used to exit the current function and send the reply</li> </ul>"},{"location":"documentation/nodes/#thoughtinstruction-address","title":"thought(instruction, ADDRESS)","text":"<p><code>autograms.nodes.thought</code> Autograms node wrapper for <code>autograms.functional.thought</code> Generates a thought response that is logged but not displayed to the user.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): Instruction to guide the thought response.</li> <li><code>ADDRESS</code> (str, optional) Only used by compiler--must be string literal or predefined constant. Used to recover position of function call in the code of an @autograms_function. Allows for GOTO and RETURNTO to jump to location in the code.</li> <li>Returns:</li> <li><code>str</code>: The generated thought.</li> </ul>"},{"location":"documentation/nodes/#silent_thoughtinstruction-address","title":"silent_thought(instruction, ADDRESS)","text":"<p><code>autograms.nodes.silent_thought</code> Autograms node wrapper for  <code>autograms.functional.silent_thought</code> Generates a thought response without logging it.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): Instruction to guide the thought response.</li> <li><code>ADDRESS</code> (str, optional) Only used by compiler--must be string literal or predefined constant. Used to recover position of function call in the code of an @autograms_function. Allows for GOTO and RETURNTO to jump to location in the code.</li> <li>Returns:</li> <li><code>str</code>: The generated thought.</li> </ul>"},{"location":"documentation/nodes/#transitiontransition_question-transitions-max_turns1addressnone-kwargs","title":"TRANSITION(transition_question, transitions, max_turns=1,ADDRESS=None, <code>**kwargs</code>)","text":"<p><code>autograms.nodes.TRANSITION</code></p> <p>Autograms node wrapper for <code>autograms.functionalTRANSITION</code> Manages a transition based on multiple-choice input.</p> <ul> <li>Parameters:</li> <li><code>transition_question</code> (str): The question prompting the transition.</li> <li><code>transitions</code> (dict): Maps choices to their corresponding addresses.</li> <li><code>max_turns</code> (int, optional): Maximum attempts for a valid response.</li> <li> <p><code>**kwargs</code>: Additional arguments for the model.</p> </li> <li> <p>Raises:</p> </li> <li><code>GoTo</code>: To jump to the target address.</li> </ul>"},{"location":"documentation/prompt_functions/","title":"Prompt functions","text":""},{"location":"documentation/prompt_functions/#prompt-and-history-functions","title":"Prompt and History Functions","text":"<p>Located in <code>autograms.functional</code>.</p>"},{"location":"documentation/prompt_functions/#set_system_prompttext","title":"set_system_prompt(text)","text":"<p><code>autograms.functional.set_system_prompt</code> Sets the system prompt in memory. The system prompt will be reset to the calling scopes system prompt when a non-root function returns.</p> <ul> <li>Parameters:</li> <li><code>text</code> (str): The system prompt text.</li> </ul>"},{"location":"documentation/prompt_functions/#append_system_prompt","title":"append_system_prompt()","text":"<p><code>autograms.functional.append_system_prompt</code> Appends the system prompt in memory. When calling a new @autograms_function, the system prompt is inherited from the calling scope. Sometimes its desiable for the system prompt within a function to be appended to the original. The system prompt then resets back to the calling scope after the inner function returns.</p> <ul> <li>Parameters:</li> <li><code>text</code> (str): The system prompt text.</li> </ul>"},{"location":"documentation/prompt_functions/#get_system_prompt","title":"get_system_prompt()","text":"<p><code>autograms.functional.get_system_prompt</code> Gets the system prompt in memory. </p> <ul> <li>Returns:</li> <li><code>text</code> (str): The system prompt text.</li> </ul>"},{"location":"documentation/prompt_functions/#get_turn_historyinstruction-max_turns-1-conv_onlyfalse","title":"get_turn_history(instruction=\"\", max_turns=-1, conv_only=False)","text":"<p><code>autograms.functional.get_turn_history</code> Retrieves the conversation history formatted for the model.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): Instruction for the current turn.</li> <li><code>max_turns</code> (int): Maximum turns to include.</li> <li> <p><code>conv_only</code> (bool): Whether to include only conversation turns.</p> </li> <li> <p>Returns:</p> </li> <li><code>tuple</code>: Input turns, output turns, and system prompt.</li> </ul>"},{"location":"documentation/prompt_functions/#extract_full_conv_history","title":"extract_full_conv_history()","text":"<p><code>autograms.functional.extract_full_conv_history</code> Extracts the conversation history between the model and the user. It ignores internal thought turns as well as the current position and scope of the stack. It is the cleanest way to extract a simple conversation history. It returns a list of dicts, where each dict has <code>\"role\"</code> (user/assistant) and <code>\"content\"</code> (reply). While autograms is generally designed so that the conversation history prompts are managed automatically, you can also extract the conversation to manage the prompts however you want and use them to call models directly. - Returns:   - <code>list[dict]</code>: A list of all conversation turns between the model and the user</p>"},{"location":"documentation/prompt_functions/#extract_last_user_reply","title":"extract_last_user_reply()","text":"<p>Extracts the last submitted user reply, or None if the conversation hasn't started. - Returns:    - <code>str</code>: last submited user reply given with <code>memory_object.add_user_reply</code>. This typically happens automatically if you pass a <code>user_reply</code> argument to <code>autogram.reply</code> or <code>autogram.apply</code></p>"},{"location":"documentation/reply_functions/","title":"Reply functions","text":""},{"location":"documentation/reply_functions/#reply-functions","title":"Reply Functions","text":"<p>Reply functions are used for giving responses in an Autograms chatbot. These functions pause the program and cause the stack to return to the point before where the first @autograms_function was called. Typically, the entry @autograms_function is wrapped using the Autogram class. A call to a reply function will cause <code>autogram.reply()</code> to return. The Memory Object tracks what line in an @autograms_function the reply came from, so that the next time autogram.reply() is called with a new user input, the program will continue from that point--including recalling any @autograms_function s that are needed to re-create the stack from where the reply was given.</p> <p>There are some similarities between reply() functions and the Python <code>input</code> function. <code>input</code> also pauses a program to wait for a reply. However with <code>input</code> or any other mechanism that pauses the program, the program must continue to run indefinitely while waiting for a response, making it impractical for most chatbot APIs that mandle multiple users. Autograms reply functions used within @autograms_functions and managed by an Autogram object can simulate the effect of the <code>input</code> command while also allowing the state of the function (and stack trace) to be serialized, saved, and reloaded from the disk or a database later.</p>"},{"location":"documentation/reply_functions/#replytext-datakwargs","title":"reply(text, data,<code>**kwargs</code>)","text":"<p><code>autograms.functional.reply</code> Raises a <code>ReplyExit</code> exception to send a direct reply.</p> <ul> <li>Parameters:</li> <li><code>text</code> (str): The reply message to the user.</li> <li> <p><code>data</code> (dict): optional data to be sent back in autograms return object sent by autogram.apply()</p> </li> <li> <p>Raises:</p> </li> <li><code>ReplyExit</code>: Used to exit the current function and send the reply.</li> </ul>"},{"location":"documentation/reply_functions/#reply_suffixinstructionkwargs","title":"reply_suffix(instruction,<code>**kwargs</code>)","text":"<p><code>autograms.functional.reply_suffix</code> Replies to the user and includes the specified suffix in the response.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): The suffix to be included in the reply.</li> <li> <p><code>data</code> (dict): optional data to be sent back in autograms return object sent by autogram.apply()</p> </li> <li> <p>Raises:</p> </li> <li><code>ReplyExit</code>: Used to exit the current function and send the reply.</li> </ul>"},{"location":"documentation/reply_functions/#reply_instructioninstructionkwargs","title":"reply_instruction(instruction,<code>**kwargs</code>)","text":"<p><code>autograms.functional.reply_instruction</code> Replies based on an instruction generated by the conversational model.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): The instruction to guide the reply.</li> <li> <p><code>data</code> (dict): optional data to be sent back in autograms return object sent by autogram.apply()</p> </li> <li> <p>Raises:</p> </li> <li><code>ReplyExit</code>: Used to exit the current function and send the reply</li> </ul>"},{"location":"documentation/reply_functions/#address-argument-in-reply-functions-advanced","title":"ADDRESS argument in reply functions (Advanced)","text":"<p>All reply functions accept an <code>ADDRESS</code> keyword argument meant for the compiler. Specifying <code>ADDRESS</code> for reply functions is optional and more advanced, but does makes it possible to change the program mid execution since it allows line numbers to be mapped. Without an <code>ADDRESS</code> defined, it will most likely cause an error to do this due to the changing line numbers. If you for instance encounter a reply you do not like--if an <code>ADDRESS</code> is defined, you can change the code and reload the previous memory object to see how this change affects the reply. If the reply is inside a nested <code>@autograms_function()</code> scope, you must also provide an <code>ADDRESS</code> argument to each <code>@autograms_function()</code> call in the stack trace. If you want your code to be able to be reloaded mid-execution with changes--you should include an <code>ADDRESS</code> key word argument to every reply function/node, and every call to an <code>@autograms_function()</code>. Note that all <code>@autograms_function()</code>s can accept an <code>ADDRESS</code> keyword, regardless of whether the wrapped function has an <code>ADDRESS</code> keyword argument.</p> <p>Technically, an <code>ADDRESS</code> for a reply function can be a target for a <code>GOTO</code> or <code>RETURNTO</code>, although it's recommended to use nodes instead for this behavior.  </p> <p><code>ADDRESS</code> is only meant for the compiler-- and each value passed as an <code>ADDRES</code>S must be unique, and be a string literal or global constant so that it can be determined at compile time. </p>"},{"location":"documentation/simulation_finetuning/","title":"AutoGRAMS Supervisors: Simulation &amp; Finetuning Guide","text":""},{"location":"documentation/simulation_finetuning/#overview","title":"Overview","text":"<p>AutoGRAMS provides tools to simulate conversations between agents and use the results to train or finetune models. This guide walks through:</p> <ul> <li>Setting up simulations between a model chatbot and a user chatbot</li> <li>Using supervisor functions to capture richer outputs</li> <li>Saving and transforming simulation data</li> <li>Running finetuning with OpenAI or Hugging Face models</li> </ul>"},{"location":"documentation/simulation_finetuning/#why-simulate-conversations","title":"Why Simulate Conversations?","text":"<p>Simulation helps generate data for two common scenarios:</p> <ol> <li>LLM Distillation \u2014 simulate high-quality conversations using GPT-4 or a complex agent, then finetune a smaller model (e.g. GPT-3.5, Mistral, Qwen) to behave similarly.</li> <li>Agent Distillation \u2014 convert a complex, multi-step AutoGRAM (with planning, thoughts, or tool use) into a simpler agent using direct primitives like <code>reply_instruction()</code> or <code>silent_thought()</code>.</li> </ol>"},{"location":"documentation/simulation_finetuning/#simulating-with-run_simulationpy","title":"Simulating with <code>run_simulation.py</code>","text":"<p>Run simulated conversations using:</p> <pre><code>python run_simulation.py \\\n  --autogram_file my_chatbot.py \\\n  --userbot_file user_agent.py \\\n  --num_turns 2 \\\n  --num_examples 10 \\\n  --save_dir simulation_data\n</code></pre>"},{"location":"documentation/simulation_finetuning/#key-arguments","title":"Key Arguments","text":"Flag Purpose <code>--autogram_file</code> File with chatbot logic (must define <code>chatbot()</code> function) <code>--userbot_file</code> File defining the user agent logic <code>--num_turns</code> Number of turns per conversation <code>--num_examples</code> Number of conversations to simulate <code>--simulation_list_file</code> Optional: predefined list of scenarios <code>--save_dir</code> Where to save <code>.pkl</code> memory outputs"},{"location":"documentation/simulation_finetuning/#what-is-a-userbot","title":"What is a Userbot?","text":"<p>A userbot can be:</p> <ul> <li>A simple echo agent:</li> </ul> <pre><code>@autograms_function()\ndef chatbot(system_prompt):\n    set_system_prompt(system_prompt)\n    while True:\n        reply_instruction(\"Reply as the user following the system prompt\")\n</code></pre> <ul> <li>A fully autonomous AutoGRAM with memory and branching logic.</li> </ul>"},{"location":"documentation/simulation_finetuning/#supervisor-functions","title":"Supervisor Functions","text":"<p>Supervisor functions let you override primitive calls (e.g., <code>silent_thought</code>) with more sophisticated multi-step reasoning during simulation.</p> <p>When <code>supervisor_mode=True</code>, you can wrap calls like this:</p> <pre><code>silent_thought(\"Summarize the article\", supervisor=reasoned_summary, article=article)\n</code></pre>"},{"location":"documentation/simulation_finetuning/#example-supervisor-function","title":"Example Supervisor Function","text":"<pre><code>from autograms.supervisors import SupervisorReturn\n\ndef reasoned_summary(instruction, article):\n    findings = silent_thought(f\"{article}\\n\\nWhat are the key findings?\", article=article)\n    rel = silent_thought(f\"{article}\\nFindings: {findings}\\nHow are they relevant?\", article=article)\n    result = silent_thought(f\"{article}\\nFinal Summary: {findings}\\nRelevance: {rel}\", article=article)\n    return SupervisorReturn(result)\n</code></pre> <p>This logic only runs during simulation. During deployment, <code>silent_thought()</code> is used directly.</p>"},{"location":"documentation/simulation_finetuning/#dpo-training-with-supervisorreturn","title":"DPO Training with SupervisorReturn","text":"<p>You can also use supervisor functions to output preferred and rejected completions:</p> <pre><code>def make_formal(instruction):\n    formal = silent_thought(f\"{instruction}. Make it formal.\")\n    informal = silent_thought(f\"{instruction}. Make it casual.\")\n    return SupervisorReturn(formal, rejected_output=informal)\n\nreply_instruction(\"Respond to the user.\", supervisor=make_formal)\n</code></pre>"},{"location":"documentation/simulation_finetuning/#custom-simulation-scenarios","title":"Custom Simulation Scenarios","text":"<p>You can define a JSON list of simulation cases:</p> <pre><code>[\n  {\"num_turns\": 2, \"chatbot_kwargs\": {\"tag\": \"A\"}, \"user_kwargs\": {}},\n  {\"num_turns\": 3, \"chatbot_kwargs\": {\"tag\": \"B\"}, \"user_kwargs\": {}}\n]\n</code></pre> <p>Then use:</p> <pre><code>--simulation_list_file my_cases.json\n</code></pre>"},{"location":"documentation/simulation_finetuning/#finetuning-with-run_finetuningpy","title":"Finetuning with <code>run_finetuning.py</code>","text":"<p>Once <code>.pkl</code> files are generated by the simulation, run:</p> <pre><code>python run_finetuning.py \\\n  --save_dir simulation_data \\\n  --model Qwen/Qwen2.5-32B-Instruct \\\n  --model_type huggingface \\\n  --finetuning_type dpo\n</code></pre>"},{"location":"documentation/simulation_finetuning/#finetuning-options","title":"Finetuning Options","text":"Option Values Purpose <code>--model_type</code> <code>huggingface</code>, <code>openai</code> Backend to train on <code>--finetuning_type</code> <code>normal</code>, <code>dpo</code> Choose standard vs. preference training <code>--prepare_data_only</code> flag Export JSONL but skip training"},{"location":"documentation/simulation_finetuning/#output-files","title":"Output Files","text":"File Contents <code>train.jsonl</code> Instruction \u2192 output pairs <code>train_dpo.jsonl</code> DPO format: prompt, chosen, rejected <p>These can be loaded into OpenAI\u2019s finetuning CLI, Hugging Face\u2019s <code>Trainer</code>, or any custom pipeline.</p>"},{"location":"documentation/simulation_finetuning/#summary-components","title":"Summary: Components","text":"Component Role <code>run_simulation.py</code> Simulates agent-user conversations, optionally with supervisors Supervisor functions Adds logic for complex reasoning or preference selection <code>run_finetuning.py</code> Extracts conversation data and calls finetuning APIs <p>AutoGRAMS lets you simulate rich interaction traces \u2014 then distill them into powerful, streamlined agents.</p>"},{"location":"documentation/structured_generation_functions/","title":"Generation functions","text":""},{"location":"documentation/structured_generation_functions/#generation-functions","title":"Generation Functions","text":"<p>Generation functions indirectly manage calls to models as well as logging of turns for future use in prompts. They can be used to produce normal string outputs as well as structured outputs via Pydantic.</p>"},{"location":"documentation/structured_generation_functions/#thoughtinstruction","title":"thought(instruction)","text":"<p><code>autograms.functional.thought</code> Generates a thought response that is logged but not displayed to the user.</p> <ul> <li>Parameters:</li> <li> <p><code>instruction</code> (str): Instruction to guide the thought response.</p> </li> <li> <p>Returns:</p> </li> <li><code>str</code>: The generated thought.</li> </ul>"},{"location":"documentation/structured_generation_functions/#silent_thoughtinstruction","title":"silent_thought(instruction)","text":"<p><code>autograms.functional.silent_thought</code> Generates a thought response without logging it.</p> <ul> <li>Parameters:</li> <li> <p><code>instruction</code> (str): Instruction to guide the thought response.</p> </li> <li> <p>Returns:</p> </li> <li><code>str</code>: The generated thought.</li> </ul>"},{"location":"documentation/structured_generation_functions/#generate_list_of_dictsinstruction-keys-kwargs","title":"generate_list_of_dicts(instruction, keys, <code>**kwargs</code>)","text":"<p><code>autograms.functional.generate_list_of_dicts</code> Generates a list of dictionaries with fixed keys and string values based on the provided instruction.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): Instruction to guide the list generation.</li> <li> <p><code>keys</code> (list[str]): List of fields for each dictionary. Each dictionary will include these fields as keys.</p> </li> <li> <p>Returns:</p> </li> <li><code>list[dict]</code>: A list of dictionaries where each dictionary has the specified keys with string values.</li> </ul>"},{"location":"documentation/structured_generation_functions/#thought_chaininstruction-chain-kwargs","title":"thought_chain(instruction, chain, <code>**kwargs</code>)","text":"<p><code>autograms.functional.thought_chain</code> Executes a series of thoughts in one call to the model, where each thought can have a different prompt.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): Instruction to guide the thought chain.</li> <li> <p><code>chain</code> (list[dict]): A list of prompts for each thought in the chain.</p> </li> <li> <p>Returns:</p> </li> <li><code>list[str]</code>: A list of generated responses, with each response corresponding to a prompt in the chain.</li> </ul>"},{"location":"documentation/structured_generation_functions/#thought_decision_chaininstruction-chain_structure-fixed_typenone-kwargs","title":"thought_decision_chain(instruction, chain_structure, fixed_type=None, <code>**kwargs</code>)","text":"<p><code>autograms.functional.thought_decision_chain</code> Executes a series of actions (thoughts or decisions) in one call to the model, using a structured output schema.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): Instruction to guide the thought-decision chain.</li> <li><code>chain_structure</code> (list[dict]): A list of actions to be performed. Each action must specify its type as either <code>thought</code> or <code>decision</code>.<ul> <li><code>thought</code> example: <code>{'type': 'thought', 'prompt': &lt;str&gt;}</code></li> <li><code>decision</code> example: <code>{'type': 'decision', 'question': &lt;str&gt;, 'choices': &lt;list[str]&gt;}</code></li> </ul> </li> <li> <p><code>fixed_type</code> (optional): Specifies a fixed type for all items in the chain. Defaults to <code>None</code>.</p> </li> <li> <p>Returns:</p> </li> <li><code>list[str]</code>: A list of results for each item in the chain. For thoughts, this will be the model output. For decisions, this will be one of the provided choices.</li> </ul>"},{"location":"documentation/structured_generation_functions/#generate_listinstruction-kwargs","title":"generate_list(instruction, <code>**kwargs</code>)","text":"<p><code>autograms.functional.generate_list</code> Generates a list based on the given instruction.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): The instruction for generation.</li> <li> <p><code>**kwargs</code>: Additional arguments for the model.</p> </li> <li> <p>Returns:</p> </li> <li><code>list[str]</code>: A list of generated items.</li> </ul>"},{"location":"documentation/structured_generation_functions/#generate_fixed_listinstruction-num_items-kwargs","title":"generate_fixed_list(instruction, num_items, <code>**kwargs</code>)","text":"<p><code>autograms.functional.generate_fixed_list</code> Generates a list with a fixed number of items.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): Instruction for generation.</li> <li><code>num_items</code> (int): Number of items in the list.</li> <li> <p><code>**kwargs</code>: Additional arguments for the model.</p> </li> <li> <p>Returns:</p> </li> <li><code>list[str]</code>: A fixed-size list.</li> </ul>"},{"location":"documentation/structured_generation_functions/#generate_fixed_dictinstruction-keys-kwargs","title":"generate_fixed_dict(instruction, keys, <code>**kwargs</code>)","text":"<p><code>autograms.functional.generate_fixed_dict</code> Generates a dictionary with fixed keys.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): Instruction for generation.</li> <li><code>keys</code> (list[str]): Keys for the dictionary.</li> <li> <p><code>**kwargs</code>: Additional arguments for the model.</p> </li> <li> <p>Returns:</p> </li> <li><code>dict</code>: A dictionary with generated values.</li> </ul>"},{"location":"documentation/structured_generation_functions/#generate_objectinstruction-obj_structure-kwargs","title":"generate_object(instruction, obj_structure, <code>**kwargs</code>)","text":"<p><code>autograms.functional.generate_object</code> Generates an object based on the given instruction and model structure.</p> <ul> <li>Parameters:</li> <li><code>instruction</code> (str): Instruction for generation.</li> <li><code>obj_structure</code> (BaseModel): Pydantic model structure.</li> <li> <p><code>**kwargs</code>: Additional arguments.</p> </li> <li> <p>Returns:</p> </li> <li><code>BaseModel</code>: Generated object.</li> </ul>"}]}